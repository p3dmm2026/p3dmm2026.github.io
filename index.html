<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>P3DMM@ICME2026</title>
  <link rel="stylesheet" href="styles.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
  <!-- Hero Section -->
  <section class="hero">
    <h1>Physical Principles for Reliable 3D Modelling in Multimedia (P3DMM)</h1>
    <p class="subtitle">IEEE International Conference on Multimedia and Expo 2026 Workshop</p>
    <p class="details">
      <i class="fa fa-calendar"></i> July 5 – 9, 2026 &nbsp;
      <i class="fa fa-map-marker"></i> Bangkok Marriott Marquis Queen’s Park, Bangkok, Thailand
    </p>
  </section>

  <!-- Main Content Container -->
  <div class="container">
    <!-- Summary Section -->
    <section id="summary">
      <h2>Summary</h2>
        <p>
          Reliable 3D modelling is a foundational capability for many multimedia applications, yet achieving metrically accurate and physically meaningful 3D representations in real-world environments remains challenging. Variations in illumination, material properties, motion, sensor configurations, and environmental conditions often undermine the robustness and interpretability of purely data-driven approaches. This workshop focuses on advancing physically informed and physically interpretable 3D modelling, learning, and perception methods that explicitly incorporate physical principles to improve reliability, consistency, and trustworthiness across multimedia scenarios. This workshop aims to provide a unified forum for discussing how such physical principles can be systematically embedded into modern learning frameworks, including neural fields, radiance models, and multimodal foundation models.
        </p>
        <p>
         In addition to physics-guided methods, the workshop welcomes contributions that integrate physical priors with diverse multimedia and sensing modalities, such as RGB-D, multi-view and video data, IMU and robotic kinematics, force and tactile sensing, acoustic measurements, and spectral or hyperspectral imaging. Particular emphasis is placed on methods that enhance physical consistency, interpretability, and measurement fidelity, enabling reliable 3D modelling for applications such as digital twins, intelligent manufacturing, robotics, biomedical imaging, and computational multimedia systems.
        </p>
    </section>


    <!-- Call for Papers Section -->
    <section id="Call For Paper">
      <h2>Call for Papers</h2>
      <p>We invite original submissions that address challenges and advances across the full spectrum of Physical Principles 3D Modelling. Topics of interest include, but are not limited to:</p>

      <ul>
        <li><b>Learning-based 3D modelling with physical principles</b><br>
        </li>
        <li><b>Physics-coherent neural fields and radiance models</b><br>
          
        </li>
        <li><b>Shape, lighting and material decomposition with physical consistency</b><br>
        
        </li>
        <li><b>Modelling contact, collision and rigid/deformable body behaviour</b><br>
          
        </li>
        <li><b>Data-driven methods enriched by physical cues</b><br>
  
        </li>
        <li><b>Reliable 3D modelling in complex multimedia environments</b><br>
          
        </li>
        <li><b>Physical cues for digital twins and manufacturing</b><br>
        </li>
        
        <li><b>Multimedia applications requiring physically interpretable 3D models</b><br>
        </li>

        <li><b>Datasets, metrics and evaluations for physics-informed 3D modelling</b><br>
        </li>
      </ul>

        <!-- Submission Links -->
      <p><b>Submission Guidance:</b> 
        <a href="https://2026.ieeeicme.org/author-information-and-submission-instructions/">
          Submit via CMT
        </a>
      </p>
      <p><b>Download CFP (PDF):</b> 
        <a href="assets/pdfs/c4p.pdf" target="_blank">
          Click here to download
        </a>
      </p>
      <p><b>Important Registration Note:</b>  
        All accepted papers need to be covered by a 
        <span style="color: red; font-weight: bold;">full registration</span>.  
      </p>
    </section>



    <!-- Speakers Section -->
    <section id="speakers">
      <h2>Keynote Speakers</h2>
      <div class="speaker-grid">
        <div class="speaker">
          <img src="assets/imgs/hjh.png" alt="Prof. Junhui Hou">
          <h3>Prof. Junhui Hou</h3>
          <p>(CityU, HKSAR)</p>
        </div>
        <div class="speaker">
          <img src="assets/imgs/zhy.jpg" alt="Prof. Huiyu Zhou">
          <h3>Prof. Huiyu Zhou</h3>
          <p>(University of Leicester, UK)</p>
        </div>
    </section>

    <!-- Schedule Section -->
  <section id="schedule">
    <h2>Schedule</h2>
    <p><b><em>Note: The schedule is for reference only and is subject to change.</em></b></p>
    <p><b><em>Join us via: https://nus-sg.zoom.us/j/4749365770?pwd=H5Fk0ZPO0rjxdewISh46d3lrP0v8lb.1</em></b></p>

    <table>
      <tr>
        <th>Time</th>
        <th>Event</th>
      </tr>

      <!-- Opening -->
      <tr>
        <td>14:00 - 14:05</td>
        <td>Welcome &amp; Session Introduction</td>
      </tr>

      <!-- Oral Session: 4 talks, each 8 min (+2 for Q&A inside block) -->
      <tr>
        <td>14:05 - 14:45</td>
        <td>Oral Session (4 contributed talks, 8+2 min each)</td>
      </tr>

      <!-- Keynote: Phoebe Chen -->
      <tr>
        <td>14:45 - 15:25</td>
        <td>Keynote: Prof. Yi-Ping Phoebe Chen (40 min)</td>
      </tr>

      <!-- Coffee Break + Posters -->
      <tr>
        <td>15:30 - 16:00</td>
        <td>Coffee Break</td>
      </tr>

      
      <!-- Invited Speaker Talk #1 -->
      <tr>
        <td>16:00 - 16:20</td>
        <td>Invited Speaker Talk #1: Dr. Zhuoyuan Li (20 min)</td>
      </tr>


      <!-- Invited Speaker Talk #2 -->
      <tr>
        <td>16:20 - 16:40</td>
        <td>Invited Speaker Talk #2: Dr. Hanyu Zhou (20 min)</td>
      </tr>

      <!-- Invited Speaker Talk #3 -->
      <tr>
        <td>16:40 - 17:00</td>
        <td>Invited Speaker Talk #3: Dr. Zixiang Zhao (20 min)</td>
      </tr>

      <!-- Awards & Closing -->
      <tr>
        <td>17:00 - 17:10</td>
        <td>Awards &amp; Closing (Best Paper, etc.)</td>
      </tr>
    </table>
</section>


  <!-- Accepted Papers Section -->
<section id="accepted-papers">
  <h2>Accepted Papers</h2>

  <table class="paper-table">
    <tr>
      <th>Title</th>
      <th>Type</th>
    </tr>
    <tr>
      <td>Exploiting Appearance Re-Emergence for Robust Visual Tracking</td>
      <td>Oral</td>
    </tr>
    <tr>
      <td>Spike Camera Image Reconstruction Based on an Efficient Spiking Transformer</td>
      <td>Oral</td>
    </tr>
    <tr>
      <td>PanoExtend: An Omnidirectional Image Super-Resolution Method Based on Spherical Expansion</td>
      <td>Oral</td>
    </tr>
    <tr>
      <td>Revisiting Intelligent Settlement and Nutritional Estimation of Small-bowl Dishes via Deep Learning</td>
      <td>Oral</td>
    </tr>
    <tr>
      <td>Seeing in the Noisy Dark: A New Real-world Benchmark and an Efficient Method for Extreme Low-light Image Enhancement</td>
      <td>Poster</td>
    </tr>
    <tr>
      <td>Point Long-Term Locality-Aware Transformer for Point Cloud Video Understanding</td>
      <td>Poster</td>
    </tr>
    <tr>
      <td>Multi-scale Dynamic Network for Document Shadow Removal</td>
      <td>Poster</td>
    </tr>
    <tr>
      <td>Memory-Augmented Continuous-Time Neural Policy for Vision-Guided Embodied Navigation</td>
      <td>Poster</td>
    </tr>
    <tr>
      <td>PanoExtend: An Omnidirectional Image Super-Resolution Method Based on Spherical Expansion</td>
      <td>Poster</td>
    </tr>
    <tr>
      <td>A Survey on Future Physical World Generation for Autonomous Driving</td>
      <td>Poster</td>
    </tr>
    <tr>
      <td>A Survey for Point Prompt of Segment Anything Model</td>
      <td>Poster</td>
    </tr>
    <tr>
      <td>Triple-Branch Fusion Module with Spatial-Frequency Cross-Attention Mechanism for Small Object Detection</td>
      <td>Poster</td>
    </tr>
  </table>
</section>



    <!-- Organizers Section -->
    <section id="organizers">
      <h2>Organizers</h2>
      <div class="organizers-carousel">
        <div class="organizer">
            <img src="assets/imgs/avatar_organizer_1.png"/>
          <h3>Zeyu Xiao</h3>
          <p>NUS, Singapore</p>
        </div>
        <div class="organizer">
          <img src="assets/imgs/avatar_organizer_2.png"/>
          <h3>Zhuoyuan Li</h3>
          <p>USTC, China</p>
        </div>
        <div class="organizer">
          <img src="assets/imgs/avatar_organizer_3.png"/>
          <h3>Xiang Chen</h3>
          <p>NJUST, China &amp; EntroVision</p>
        </div>
        <div class="organizer">
          <img src="assets/imgs/avatar_organizer_4.png"/>
          <h3>Cong Zhang</h3>
          <p>CUHK, HKSAR</p>
        </div>
        <div class="organizer">
          <img src="assets/imgs/avatar_organizer_5.png"/>
          <h3>Hadi Amirpour</h3>
          <p>University of Klagenfurt, Austria</p>
        </div>
        <div class="organizer">
          <img src="assets/imgs/avatar_organizer_6.png"/>
          <h3>Yakun Ju</h3>
          <p>University of Leicester, UK</p>
        </div>
        <div class="organizer">
          <img src="assets/imgs/avatar_organizer_7.png"/>
          <h3>Zhiwei Xiong</h3>
          <p>USTC, China</p>
        </div>
        <div class="organizer">
          <img src="assets/imgs/avatar_organizer_8.png"/>
          <h3>Kin-Man Lam</h3>
          <p>PolyU, HKSAR</p>
        </div>
      </div>
    </section>


    <!-- Contact Section -->
    <section id="sponsors">
      <h2>Sponsors</h2>
        <p>
          This workshop is proudly sponsored by 
          <a href="https://lowlevelcv.com/" target="_blank">EntroVision</a>.
        </p>
    </section>

    <!-- Contact Section -->
    <section id="contact">
      <h2>Contact</h2>
      <p>For further inquiries, please email us at
        <script>
          const u = "zeyuxiao";
          const d = "nus.edu.sg";
          document.write(`<a href="mailto:${u}@${d}">${u}@${d}</a>`);
        </script>.
      </p>
    </section>
  </div>
</body>
</html>
